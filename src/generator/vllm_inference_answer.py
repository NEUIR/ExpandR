from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import argparse
import datasets
import jsonlines
from tqdm import tqdm
from promptor import Promptor
import os

def generate_ans_for_dpo(args):
    max_batch_size = 1
    model_path = args.model_name_or_path
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    llm = LLM(model=model_path, trust_remote_code=True, gpu_memory_utilization=0.95)
    
    query_path = args.query_path
    queries = datasets.load_dataset('json', data_files=[query_path], split="train")
    query_texts = [query['query'] for query in queries]   
    positive_texts = [query['positive'] for query in queries]
    length = len(query_texts)
    print(length)
    
    sampling_params = SamplingParams(temperature=1, top_p=0.9, max_tokens=512)
    prompter = Promptor(task=args.task_type) 
    
    outfile_dir = args.outfile_dir
    save_file_name = f"gen_answer.jsonl"
    save_file = os.path.join(outfile_dir, save_file_name) 
     
    
    with jsonlines.open(save_file, 'w') as writer:
        for i in tqdm(range(0, length, max_batch_size), leave=False, desc="Generating documents"):
            j = min(i + max_batch_size, length)
            queries = query_texts[i: j]
            positive = positive_texts[i: j]
            prompts_list = []
            for query, passage in zip(queries, positive):
                prompt = prompter.build_prompt(query, passage)
                user_input = [ {"role": "user", "content": prompt},]
                user_input = tokenizer.apply_chat_template(user_input, add_generation_prompt=True, tokenize=False)
                prompts_list.append(user_input)
               
            outputs = llm.generate(prompts_list, sampling_params)
            output = outputs[0].outputs[0].text
            if output == '':
                outputs = llm.generate(prompts_list, sampling_params)
            
            for output in outputs:
                output_data = {
                    "answer": output.outputs[0].text
                }
                writer.write(output_data)
            
           


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name_or_path", type=str, 
                        default=None,
                        help="The path of model file."
                        )
    parser.add_argument("--query_path", type=str, 
                        default=None,
                        help="The path of queries where the model generates corresponding answers based on the query."
                        )
    parser.add_argument("--task_type", type=str, 
                        default=None,
                        help="prompt type."
                        )
    
    parser.add_argument("--outfile_dir", action='store_true',
                        help="Save answer data dir generated by llm."
                        )

    args = parser.parse_args()
    generate_ans_for_dpo(args)

if __name__ == "__main__":
    main()
