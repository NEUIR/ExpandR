from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import argparse
import sys
import datasets
import jsonlines
from tqdm import tqdm
from promptor import Promptor
import os

def post_process(in_str):
    patterns = [
        'Here is a passage to answer the question:',
        'Here is a passage that answers the question:',
        'Here is a passage answering the question:',
        "Here's a passage that attempts to answer the question:",
        "Here's a passage that answers the question:",
        "Here's a passage that answers your question:",
        "Here's a possible passage:",
        "Here is a possible passage:",
        "Here is a potential passage:",
        "Here's a potential passage:",
        "Here's a passage:",
        "Here is the passage:",
        "Here's the passage:"
    ]
    for pattern in patterns:
        if pattern in in_str:
            in_str = in_str.split(pattern)[1]
            break
    return in_str.strip()


def generate_doc_for_supervised(args):    
    max_batch_size = 1024
    model_path = args.model_name_or_path
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    llm = LLM(model=model_path, trust_remote_code=True, gpu_memory_utilization=0.95)
    
    query_path = args.query_path
    queries = datasets.load_dataset('json', data_files=[query_path], split="train")
    query_texts = [query['query'] for query in queries]         
    length = len(query_texts)
    print(length)
    
    sampling_params = SamplingParams(temperature=1, top_p=0.9, max_tokens=512)
    prompter = Promptor(task=args.task_type) 
    
    outfile_dir = args.outfile_dir
    save_file_name = f"gen_doc.jsonl"
    save_file = os.path.join(outfile_dir, save_file_name)       
        
    
    with jsonlines.open(save_file, 'w') as writer:
        for i in tqdm(range(0, length, max_batch_size), leave=False, desc="Generating documents"):
            j = min(i + max_batch_size, length)
            prompts = query_texts[i: j]
            prompts_list = []
            for prompt in prompts:
                passage = ''
                prompt = prompter.build_prompt(prompt, passage)
                user_input = [ {"role": "user", "content": prompt},]
                user_input = tokenizer.apply_chat_template(user_input, add_generation_prompt=True, tokenize=False)
                prompts_list.append(user_input)
               
            outputs = llm.generate(prompts_list, sampling_params)
            for output in outputs:
                passage = output.outputs[0].text
                passage = post_process(passage)
                output_data = {
                    "passage": output.outputs[0].text
                }
                writer.write(output_data)
            
    out_file_name = f"supervised_data.jsonl"
    outfile = os.path.join(outfile_dir, out_file_name)
    with jsonlines.open(query_path,'r') as reader1, jsonlines.open(save_file, "r") as reader2, jsonlines.open(outfile, "w") as writer:
        for in_data1, in_data2 in zip(reader1, reader2):
            outdata = {}
            query = in_data1['query']
            outdata['query'] = query
            
            doc = in_data2["passage"]
            outdata['pseudo_doc'] = doc
            
            pos = []
            pos.append(in_data1['positive'])
            outdata['pos'] = pos
            
            neg = []
            neg.append(in_data1['negative'])
            outdata['neg'] = neg
            
            writer.write(outdata)
               


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name_or_path", type=str, 
                        default=None,
                        help="The path of model file."
                        )
    parser.add_argument("--query_path", type=str, 
                        default=None,
                        help="The path of queries where the model generates corresponding documents based on the query."
                        )
    parser.add_argument("--task_type", type=str, 
                        default=None,
                        help="prompt type."
                        )
    
    parser.add_argument("--outfile_dir", type=str,
                        help="supervised training data generated by llm."
                        )

    args = parser.parse_args()
    generate_doc_for_supervised(args)

if __name__ == "__main__":
    main()


